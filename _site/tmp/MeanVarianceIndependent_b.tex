\documentclass[14pt]{extarticle}
\usepackage[margin=.4in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{bm}
\title{Mean \& Variance of Sum of Two Independent Variables}
\author{Premmi}	
\date{\today}
\begin{document}    
\maketitle
\begin{flushleft}
\begin{large}
Suppose $x$ and $y$ are two statistically independent variables. Then their mean and variance are given by
\begin{align}
\mathbb{E}[x + y] &= \mathbb{E}[x] + \mathbb{E}[y]\\
\mathrm{var}[x + y] &= \mathrm{var}[x] + \mathrm{var}[y]
\end{align}
\begin{proof}
Since the two variables $x$ and $y$ are independent, their joint distribution $f_{x,y}(x,y)$ factorizes into $f_{x}(x)f_{y}(y)$. Hence,
\begin{align}
\mathbb{E}[x + y] &= \int\!\!\!\!\int (x + y) f_{x,y}(x, y)\, \mathrm{d}x \, \mathrm{d}y\\
&= \int\!\!\!\!\int (x + y) f_{x}(x) f_{y}(y) \,\mathrm{d}x \,\mathrm{d}y\\
&= \int\!\!\!\!\int \left\lbrace x\, f_{x}(x) f_{y}(y) \,\mathrm{d}x \,\mathrm{d}y + y\, f_{x}(x) f_{y}(y) \,\mathrm{d}x \,\mathrm{d}y \right\rbrace\\
&= \int\!\!\!\!\int  x\, f_{x}(x) f_{y}(y) \,\mathrm{d}x \,\mathrm{d}y \,+ \int\!\!\!\!\int  y\, f_{x}(x) f_{y}(y) \,\mathrm{d}x \,\mathrm{d}y\\
&= \int\! x\, f_{x}(x) \,\mathrm{d}x \int\! f_{y}(y)\,\mathrm{d}y\, + \int\! y\, f_{y}(y) \,\mathrm{d}y \int\! f_{x}(x)\,\mathrm{d}x\\
&= \int\! x\, f_{x}(x) \,\mathrm{d}x\, + \int\! y\, f_{y}(y) \,\mathrm{d}y\\
&= \mathbb{E}[x] + \mathbb{E}[y] 
\end{align}
as required. $(8)$ follows from the fact that the pdf integrates to 1 and $(9)$ follows from the definition of expectation.
\begin{align}
\mathrm{var}[x + y] &= \mathbb{E}[\,(x + y) - \mathbb{E}[x + y]\,]^2\\
&= \mathbb{E}[\,(x + y)^2 - 2\, (x + y)\,\mathbb{E}[x + y] + \mathbb{E}[x + y]^2\,]\\
&= \mathbb{E}[\,(x + y)^2\,] - 2\,\mathbb{E}[x + y]^2 + \mathbb{E}[x + y]^2\\
&= \mathbb{E}[\,(x + y)^2\,] - \mathbb{E}[x + y]^2\\
&= \mathbb{E}[x^2] + \mathbb{E}[y^2] + 2\,\mathbb{E}[xy] - (\,\mathbb{E}[x] + \mathbb{E}[y]\,)^2\\\
&= \mathbb{E}[x^2] + \mathbb{E}[y^2] + 2\,\mathbb{E}[xy] - \mathbb{E}[x]^2 - \mathbb{E}[y]^2 - 2\,\mathbb{E}[x]\,\mathbb{E}[y]\\
&= \mathbb{E}[x^2] + \mathbb{E}[y^2] + 2\,\mathbb{E}[x]\,\mathbb{E}[y] - \mathbb{E}[x]^2 - \mathbb{E}[y]^2 - 2\,\mathbb{E}[x]\,\mathbb{E}[y]\\
&= \mathbb{E}[x^2] - \mathbb{E}[x]^2 + \mathbb{E}[y^2] - \mathbb{E}[y]^2\\
&= \mathrm{var}[x] + \mathrm{var}[y]
\end{align}
as required. Also, $\mathrm{cov}(x, y) = \mathbb{E}[xy] - \mathbb{E}[x]\,\mathbb{E}[y]$ and $\mathrm{cov}(x, y) = 0$ since x and y are independent, it follows that $\mathbb{E}[xy] = \mathbb{E}[x]\,\mathbb{E}[y]$.\\
\end{proof}
\end{large}
\end{flushleft}
\begin{thebibliography}{9}
\bibitem{christopherbishop} 
Christopher M. Bishop. 
\textit{ Pattern Recognition and Machine Learning}.  
\textbf{Exercise 1.10}
\end{thebibliography}
\end{document} 